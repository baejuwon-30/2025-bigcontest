# -*- coding: utf-8 -*-
"""회복탄력성_지수_계산.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sq1wyyZfC4--ohC9gB2F2rnHX92qYZeN

# 📌 데이터셋 불러오기 및 전처리
"""

import numpy as np
import pandas as pd

df1 = pd.read_csv('sample_data/big_data_set1_f.csv', encoding='cp949')
df2 = pd.read_csv('sample_data/big_data_set2_f.csv', encoding='cp949')
df3 = pd.read_csv('sample_data/big_data_set3_f.csv', encoding='cp949')

key_cols = ['ENCODED_MCT', 'TA_YM']

merge1 = pd.merge(df1, df2, on='ENCODED_MCT', how='inner')

merged_df = pd.merge(merge1, df3, on=['ENCODED_MCT', 'TA_YM'], how='inner')

merged_df.shape

merged_df.head()

# 2023년 1월 ~ 2024년 12월(24개월) 기준년월 목록 생성
date_range = pd.date_range(start='2023-01-01', end='2024-12-01', freq='MS').strftime('%Y%m').tolist()
date_set = set(date_range)

# groupby로 각 가맹점별 TA_YM 목록 추출
def check_full_months(group):
    months = set(group['TA_YM'].astype(str))
    return months == date_set

# 각 가맹점별로 24개월 데이터가 모두 있는지 검사
exist_all_months = merged_df.groupby('ENCODED_MCT').apply(check_full_months)
full_merchant_ids = exist_all_months[exist_all_months].index.tolist()  # 24개월 모두 존재하는 가맹점 리스트

print('전체 기간 데이터 존재하는 가맹점 수:', len(full_merchant_ids))

filtered_df = merged_df[merged_df['ENCODED_MCT'].isin(full_merchant_ids)].copy()

filtered_df.shape

# ENCODED_MCT와 TA_YM 기준으로 정렬
filtered_df_sorted = filtered_df.sort_values(by=['ENCODED_MCT', 'TA_YM'], ascending=[True, True]).reset_index(drop=True)

filtered_df_sorted.head()

column_map = {
    'ENCODED_MCT': '가맹점구분번호',
    'MCT_BSE_AR': '가맹점주소',
    'MCT_NM': '가맹점명',
    'MCT_BRD_NUM': '브랜드구분코드',
    'MCT_SIGUNGU_NM': '가맹점지역',
    'HPSN_MCT_ZCD_NM': '업종',
    'HPSN_MCT_BZN_CD_NM': '상권',
    'ARE_D': '개설일',
    'MCT_ME_D': '폐업일',
    'TA_YM': '기준년월',
    'MCT_OPE_MS_CN': '가맹점 운영개월수 구간',
    'RC_M1_SAA': '매출금액 구간',
    'RC_M1_TO_UE_CT': '매출건수 구간',
    'RC_M1_UE_CUS_CN': '유니크 고객 수 구간',
    'RC_M1_AV_NP_AT': '객단가 구간',
    'APV_CE_RAT': '취소율 구간',
    'DLV_SAA_RAT': '배달매출금액 비율',
    'M1_SME_RY_SAA_RAT': '동일 업종 매출금액 비율',
    'M1_SME_RY_CNT_RAT': '동일 업종 매출건수 비율',
    'M12_SME_RY_SAA_PCE_RT': '동일 업종 내 매출수위비율',
    'M12_SME_BZN_SAA_PCE_RT': '동일 상권 내 매출수위비율',
    'M12_SME_RY_ME_MCT_RAT': '동일 업종 내 해지 가맹점 비중',
    'M12_SME_BZN_ME_MCT_RAT': '동일 상권 내 해지 가맹점 비중',
    'M12_MAL_1020_RAT': '남성 20대이하',
    'M12_MAL_30_RAT': '남성 30대',
    'M12_MAL_40_RAT': '남성 40대',
    'M12_MAL_50_RAT': '남성 50대',
    'M12_MAL_60_RAT': '남성 60대이상',
    'M12_FME_1020_RAT': '여성 20대이하',
    'M12_FME_30_RAT': '여성 30대',
    'M12_FME_40_RAT': '여성 40대',
    'M12_FME_50_RAT': '여성 50대',
    'M12_FME_60_RAT': '여성 60대이상',
    'MCT_UE_CLN_REU_RAT': '재방문 고객 비중',
    'MCT_UE_CLN_NEW_RAT': '신규 고객 비중',
    'RC_M1_SHC_RSD_UE_CLN_RAT': '거주 인구 고객 비율',
    'RC_M1_SHC_WP_UE_CLN_RAT': '직장 이용 고객 비율',
    'RC_M1_SHC_FLP_UE_CLN_RAT': '유동인구 이용 고객 비율'
}


filtered_df_sorted = filtered_df_sorted.rename(columns=column_map)

filtered_df_sorted.head()

final_df = filtered_df_sorted

print(list(final_df.columns))

cat_cols_kor = [
    '가맹점 운영개월수 구간',
    '매출금액 구간',
    '매출건수 구간',
    '유니크 고객 수 구간',
    '객단가 구간',
    '취소율 구간'
]

for col in cat_cols_kor:
    unique_vals = final_df[col].nunique(dropna=False)
    print(f"'{col}' 컬럼의 고유 범주 수: {unique_vals}")

cat_cols_kor = [
    '가맹점 운영개월수 구간',
    '매출금액 구간',
    '매출건수 구간',
    '유니크 고객 수 구간',
    '객단가 구간',
    '취소율 구간'
]

for col in cat_cols_kor:
    categories = final_df[col].unique()
    print(f"'{col}' 컬럼의 범주 종류: {categories}")

# 범주별 순서대로 등수 매핑
ordinal_map = {
    '가맹점 운영개월수 구간': {
        '1_10%이하': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%초과(하위 10% 이하)': 6
    },
    '매출금액 구간': {
        '1_10%이하': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%초과(하위 10% 이하)': 6
    },
    '매출건수 구간': {
        '1_10%이하': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%초과(하위 10% 이하)': 6
    },
    '유니크 고객 수 구간': {
        '1_10%이하': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%초과(하위 10% 이하)': 6
    },
    '객단가 구간': {
        '1_10%이하': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%초과(하위 10% 이하)': 6
    },
    '취소율 구간': {
        '1_상위1구간': 1,
        '2_상위2구간': 2,
        '3_상위3구간': 3,
        '4_상위4구간': 4,
        '5_상위5구간': 5,
        '6_상위6구간(하위1구간)': 6,
        'nan': 0,  # 결측값은 0
        None: 0
    }
}

for col, mapping in ordinal_map.items():
    final_df[col] = final_df[col].map(mapping)

# 변환 후 값 확인
final_df[list(ordinal_map.keys())].head()

# 치환할 변수명 리스트 예시(한글 컬럼명 기준, 필요에 따라 수정)
cols_to_replace = [
    '배달매출금액 비율', '남성 20대이하', '남성 30대', '남성 40대', '남성 50대', '남성 60대이상',
    '여성 20대이하', '여성 30대', '여성 40대', '여성 50대', '여성 60대이상',
    '재방문 고객 비중', '신규 고객 비중', '거주 인구 고객 비율', '직장 이용 고객 비율', '유동인구 이용 고객 비율',
    '동일 상권 내 해지 가맹점 비중'
    # 필요한 변수명 추가
]

for col in cols_to_replace:
    final_df[col] = final_df[col].replace([-999999.9, -999999, -99999.9, -99999], np.nan)

# 변환 결과 확인
print(final_df[cols_to_replace].isnull().sum())

# 각 컬럼별 결측치 개수 출력
print(final_df.isnull().sum())

drop_cols = ['브랜드구분코드', '상권', '폐업일', '배달매출금액 비율', '동일 상권 내 해지 가맹점 비중']
final_df = final_df.drop(columns=drop_cols)

final_df = final_df.dropna()

final_df.shape

final_df = final_df.reset_index(drop=True)

print(final_df.isnull().sum().sum())

"""# 📌 회복탄력성 지수 반영"""

final_df.info()

grouped = final_df.groupby('가맹점구분번호')

"""## 1) 재무적 탄력성"""

def calc_financial_resilience(group):
    sales = group['매출금액 구간']
    std_sales = np.std(sales.pct_change().dropna())  # 월별 변동률 표준편차
    loyal_base = group['거주 인구 고객 비율'].mean()
    repeat_ratio = group['재방문 고객 비중'].mean()
    # 점수화: (예시 기준, 실제 프로젝트 기준에 맞게 조정)
    # 매출 변동 안정성 (작을수록 높음)
    sales_score = np.select(
        [std_sales < 0.1, std_sales < 0.3],
        [5, 3], default=1
    )
    # 거주 고객 비율
    loyal_score = np.select(
        [loyal_base >= 0.2, loyal_base >= 0.1],
        [5, 3], default=1
    )
    # 재방문률
    repeat_score = np.select(
        [repeat_ratio >= 0.05, repeat_ratio >= 0.02],
        [5, 3], default=1
    )
    return pd.Series({
        'financial_score': (sales_score + loyal_score + repeat_score) / 3
    })

"""## 2) 운영적 탄력성"""

def calc_operational_resilience(group):
    potential_expansion = group['유동인구 이용 고객 비율'].mean()
    sales_growth = (group['매출금액 구간'].values[-1] - group['매출금액 구간'].values[0]) / max(1, group['매출금액 구간'].values[0])
    transaction_std = np.std(group['매출건수 구간'])
    potential_score = np.select(
        [potential_expansion > 0.15, potential_expansion > 0.05],
        [5, 3], default=1
    )
    profit_score = np.select(
        [sales_growth > 0.0, sales_growth > -0.1],
        [5, 3], default=1
    )
    transaction_score = np.select(
        [transaction_std <= 0.1 * group['매출건수 구간'].mean(), transaction_std <= 0.3 * group['매출건수 구간'].mean()],
        [5, 3], default=1
    )
    return pd.Series({
        'operational_score': (potential_score + profit_score + transaction_score) / 3
    })

"""## 3) 관계적 탄력성"""

def calc_relational_resilience(group):
    retention = group['재방문 고객 비중'].mean()
    # 이 예시에서는 상권관계성, 위기적응성 관련 변수가 없으므로 retention 점수만 적용
    retention_score = np.select(
        [retention >= 0.05, retention >= 0.02],
        [5, 3], default=1
    )
    # 추가정보 있다면 2-3개 변수 평균
    return pd.Series({
        'relational_score': retention_score  # 단일 변수 예시
    })

"""## 4) 최종 회복탄력성 지수 계산"""

financial = grouped.apply(calc_financial_resilience)
operational = grouped.apply(calc_operational_resilience)
relational = grouped.apply(calc_relational_resilience)

result = pd.concat([financial, operational, relational], axis=1)

# 최종 회복탄력성 지수
result['회복탄력성지수'] = (
    result['financial_score'] * 0.4 +
    result['operational_score'] * 0.35 +
    result['relational_score'] * 0.25
)

# 원본 데이터프레임에 merge
final_df = final_df.merge(
    result[['financial_score', 'operational_score', 'relational_score', '회복탄력성지수']],
    left_on='가맹점구분번호', right_index=True, how='left'
)

final_df.head()

df_selected = final_df[['가맹점구분번호', 'financial_score', 'operational_score', 'relational_score', '회복탄력성지수']]

df_selected.head()

print(df_selected.dtypes)

for col in df_selected.columns:
    print(
        col,
        df_selected[col].apply(lambda x: isinstance(x, (np.ndarray, list))).sum(),
        "건이 배열이나 리스트"
    )

def safe_float(x):
    if isinstance(x, (np.ndarray, list)):
        # ndarray일 경우, 0차원이면 item() 사용, 1차원이면 [0] 사용
        if isinstance(x, np.ndarray):
            if x.ndim == 0:
                return float(x.item())
            else:
                return float(x[0])
        else: # list라면
            return float(x[0])
    else:
        return float(x)

df_selected['relational_score'] = df_selected['relational_score'].apply(safe_float)

df_unique = df_selected.drop_duplicates()

df_unique.head()

df_unique.info()

df_unique.describe()

df_unique.to_csv('/content/sample_data/resilience_all.csv', index=False, encoding='utf-8-sig')

# 기준년월 오름차순 정렬 (202301~202412 사이 데이터만)
# start_ym = 202301
# end_ym = 202412

# cond = (final_df['기준년월'] >= start_ym) & (final_df['기준년월'] <= end_ym)
# filtered = final_df[cond].copy()

# 가맹점별로 기준년월 오름차순 정렬
# final_df = filtered.sort_values(['가맹점구분번호', '기준년월']).reset_index(drop=True)

# final_df.head()

# cols = ['가맹점구분번호', 'financial_score', 'operational_score', 'relational_score', '회복탄력성지수']
# resilience_all = final_df[cols]
# resilience_all.to_csv('resilience_all.csv', index=False)

print"(

"""# 📌 '가맹점명'을 임베딩하여 벡터디비 만들기"""

!pip install langchain langchain-community langchain-openai chromadb tiktoken -q

from langchain.document_loaders import PyMuPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain.embeddings import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

sample_df = final_df.iloc[::24].copy()

text_lst = []
store_nm= []
for idx, row in sample_df.iterrows():
    store_nm.append(row['가맹점명'])
    text = (
        f"구분번호 {row['가맹점구분번호']}, "
        f"주소 {row['가맹점주소']}, "
        f"가맹점명 {row['가맹점명']}, "
        f"지역 {row['가맹점지역']}, "
        f"업종 {row['업종']}, "
        f"개설일 {row['개설일']}, "
        f"기준년월 {row['기준년월']}, "
        f"운영개월수 구간 {row['가맹점 운영개월수 구간']}개월, "
        f"매출금액 구간 {row['매출금액 구간']}, "
        f"매출건수 구간 {row['매출건수 구간']}, "
        f"유니크 고객 수 구간 {row['유니크 고객 수 구간']}, "
        f"객단가 구간 {row['객단가 구간']}, "
        f"취소율 구간 {row['취소율 구간']:.1f}. "
        f"동일 업종 매출금액 비율 {row['동일 업종 매출금액 비율']:.1f}, "
        f"동일 업종 매출건수 비율 {row['동일 업종 매출건수 비율']:.1f}, "
        f"동일 업종 내 매출수위비율 {row['동일 업종 내 매출수위비율']:.1f}, "
        f"동일 상권 내 매출수위비율 {row['동일 상권 내 매출수위비율']:.1f}, "
        f"동일 업종 내 해지 가맹점 비중 {row['동일 업종 내 해지 가맹점 비중']:.1f}"
    )
    text_lst.append(text)

vectorstore = Chroma(collection_name="store_collection2", embedding_function=embedding_model)
vectorstore.add_texts(texts=store_nm, metadatas=[{"description": d} for d in text_lst])

import os
def load_api_keys(filepath="api_key.txt"):
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if line and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key.strip()] = value.strip()



# API 키 로드 및 환경변수 설정
load_api_keys('sample_data/api_key.txt')

"""### 쿼리 검색"""

query_name = "목포**"
top_k = 3
results = vectorstore.similarity_search(query_name, k=top_k)

for i in range(top_k):
  print(results[i].metadata['description'])

store_nm