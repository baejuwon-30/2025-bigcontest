# -*- coding: utf-8 -*-
"""íšŒë³µíƒ„ë ¥ì„±_ì§€ìˆ˜_ê³„ì‚°.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sq1wyyZfC4--ohC9gB2F2rnHX92qYZeN

# ğŸ“Œ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
"""

import numpy as np
import pandas as pd

df1 = pd.read_csv('sample_data/big_data_set1_f.csv', encoding='cp949')
df2 = pd.read_csv('sample_data/big_data_set2_f.csv', encoding='cp949')
df3 = pd.read_csv('sample_data/big_data_set3_f.csv', encoding='cp949')

key_cols = ['ENCODED_MCT', 'TA_YM']

merge1 = pd.merge(df1, df2, on='ENCODED_MCT', how='inner')

merged_df = pd.merge(merge1, df3, on=['ENCODED_MCT', 'TA_YM'], how='inner')

merged_df.shape

merged_df.head()

# 2023ë…„ 1ì›” ~ 2024ë…„ 12ì›”(24ê°œì›”) ê¸°ì¤€ë…„ì›” ëª©ë¡ ìƒì„±
date_range = pd.date_range(start='2023-01-01', end='2024-12-01', freq='MS').strftime('%Y%m').tolist()
date_set = set(date_range)

# groupbyë¡œ ê° ê°€ë§¹ì ë³„ TA_YM ëª©ë¡ ì¶”ì¶œ
def check_full_months(group):
    months = set(group['TA_YM'].astype(str))
    return months == date_set

# ê° ê°€ë§¹ì ë³„ë¡œ 24ê°œì›” ë°ì´í„°ê°€ ëª¨ë‘ ìˆëŠ”ì§€ ê²€ì‚¬
exist_all_months = merged_df.groupby('ENCODED_MCT').apply(check_full_months)
full_merchant_ids = exist_all_months[exist_all_months].index.tolist()  # 24ê°œì›” ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê°€ë§¹ì  ë¦¬ìŠ¤íŠ¸

print('ì „ì²´ ê¸°ê°„ ë°ì´í„° ì¡´ì¬í•˜ëŠ” ê°€ë§¹ì  ìˆ˜:', len(full_merchant_ids))

filtered_df = merged_df[merged_df['ENCODED_MCT'].isin(full_merchant_ids)].copy()

filtered_df.shape

# ENCODED_MCTì™€ TA_YM ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
filtered_df_sorted = filtered_df.sort_values(by=['ENCODED_MCT', 'TA_YM'], ascending=[True, True]).reset_index(drop=True)

filtered_df_sorted.head()

column_map = {
    'ENCODED_MCT': 'ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸',
    'MCT_BSE_AR': 'ê°€ë§¹ì ì£¼ì†Œ',
    'MCT_NM': 'ê°€ë§¹ì ëª…',
    'MCT_BRD_NUM': 'ë¸Œëœë“œêµ¬ë¶„ì½”ë“œ',
    'MCT_SIGUNGU_NM': 'ê°€ë§¹ì ì§€ì—­',
    'HPSN_MCT_ZCD_NM': 'ì—…ì¢…',
    'HPSN_MCT_BZN_CD_NM': 'ìƒê¶Œ',
    'ARE_D': 'ê°œì„¤ì¼',
    'MCT_ME_D': 'íì—…ì¼',
    'TA_YM': 'ê¸°ì¤€ë…„ì›”',
    'MCT_OPE_MS_CN': 'ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„',
    'RC_M1_SAA': 'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„',
    'RC_M1_TO_UE_CT': 'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„',
    'RC_M1_UE_CUS_CN': 'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„',
    'RC_M1_AV_NP_AT': 'ê°ë‹¨ê°€ êµ¬ê°„',
    'APV_CE_RAT': 'ì·¨ì†Œìœ¨ êµ¬ê°„',
    'DLV_SAA_RAT': 'ë°°ë‹¬ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨',
    'M1_SME_RY_SAA_RAT': 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨',
    'M1_SME_RY_CNT_RAT': 'ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨',
    'M12_SME_RY_SAA_PCE_RT': 'ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨',
    'M12_SME_BZN_SAA_PCE_RT': 'ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨',
    'M12_SME_RY_ME_MCT_RAT': 'ë™ì¼ ì—…ì¢… ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘',
    'M12_SME_BZN_ME_MCT_RAT': 'ë™ì¼ ìƒê¶Œ ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘',
    'M12_MAL_1020_RAT': 'ë‚¨ì„± 20ëŒ€ì´í•˜',
    'M12_MAL_30_RAT': 'ë‚¨ì„± 30ëŒ€',
    'M12_MAL_40_RAT': 'ë‚¨ì„± 40ëŒ€',
    'M12_MAL_50_RAT': 'ë‚¨ì„± 50ëŒ€',
    'M12_MAL_60_RAT': 'ë‚¨ì„± 60ëŒ€ì´ìƒ',
    'M12_FME_1020_RAT': 'ì—¬ì„± 20ëŒ€ì´í•˜',
    'M12_FME_30_RAT': 'ì—¬ì„± 30ëŒ€',
    'M12_FME_40_RAT': 'ì—¬ì„± 40ëŒ€',
    'M12_FME_50_RAT': 'ì—¬ì„± 50ëŒ€',
    'M12_FME_60_RAT': 'ì—¬ì„± 60ëŒ€ì´ìƒ',
    'MCT_UE_CLN_REU_RAT': 'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘',
    'MCT_UE_CLN_NEW_RAT': 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘',
    'RC_M1_SHC_RSD_UE_CLN_RAT': 'ê±°ì£¼ ì¸êµ¬ ê³ ê° ë¹„ìœ¨',
    'RC_M1_SHC_WP_UE_CLN_RAT': 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨',
    'RC_M1_SHC_FLP_UE_CLN_RAT': 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'
}


filtered_df_sorted = filtered_df_sorted.rename(columns=column_map)

filtered_df_sorted.head()

final_df = filtered_df_sorted

print(list(final_df.columns))

cat_cols_kor = [
    'ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„',
    'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„',
    'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„',
    'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„',
    'ê°ë‹¨ê°€ êµ¬ê°„',
    'ì·¨ì†Œìœ¨ êµ¬ê°„'
]

for col in cat_cols_kor:
    unique_vals = final_df[col].nunique(dropna=False)
    print(f"'{col}' ì»¬ëŸ¼ì˜ ê³ ìœ  ë²”ì£¼ ìˆ˜: {unique_vals}")

cat_cols_kor = [
    'ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„',
    'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„',
    'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„',
    'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„',
    'ê°ë‹¨ê°€ êµ¬ê°„',
    'ì·¨ì†Œìœ¨ êµ¬ê°„'
]

for col in cat_cols_kor:
    categories = final_df[col].unique()
    print(f"'{col}' ì»¬ëŸ¼ì˜ ë²”ì£¼ ì¢…ë¥˜: {categories}")

# ë²”ì£¼ë³„ ìˆœì„œëŒ€ë¡œ ë“±ìˆ˜ ë§¤í•‘
ordinal_map = {
    'ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„': {
        '1_10%ì´í•˜': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
    },
    'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„': {
        '1_10%ì´í•˜': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
    },
    'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„': {
        '1_10%ì´í•˜': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
    },
    'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„': {
        '1_10%ì´í•˜': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
    },
    'ê°ë‹¨ê°€ êµ¬ê°„': {
        '1_10%ì´í•˜': 1,
        '2_10-25%': 2,
        '3_25-50%': 3,
        '4_50-75%': 4,
        '5_75-90%': 5,
        '6_90%ì´ˆê³¼(í•˜ìœ„ 10% ì´í•˜)': 6
    },
    'ì·¨ì†Œìœ¨ êµ¬ê°„': {
        '1_ìƒìœ„1êµ¬ê°„': 1,
        '2_ìƒìœ„2êµ¬ê°„': 2,
        '3_ìƒìœ„3êµ¬ê°„': 3,
        '4_ìƒìœ„4êµ¬ê°„': 4,
        '5_ìƒìœ„5êµ¬ê°„': 5,
        '6_ìƒìœ„6êµ¬ê°„(í•˜ìœ„1êµ¬ê°„)': 6,
        'nan': 0,  # ê²°ì¸¡ê°’ì€ 0
        None: 0
    }
}

for col, mapping in ordinal_map.items():
    final_df[col] = final_df[col].map(mapping)

# ë³€í™˜ í›„ ê°’ í™•ì¸
final_df[list(ordinal_map.keys())].head()

# ì¹˜í™˜í•  ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ ì˜ˆì‹œ(í•œê¸€ ì»¬ëŸ¼ëª… ê¸°ì¤€, í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
cols_to_replace = [
    'ë°°ë‹¬ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë‚¨ì„± 20ëŒ€ì´í•˜', 'ë‚¨ì„± 30ëŒ€', 'ë‚¨ì„± 40ëŒ€', 'ë‚¨ì„± 50ëŒ€', 'ë‚¨ì„± 60ëŒ€ì´ìƒ',
    'ì—¬ì„± 20ëŒ€ì´í•˜', 'ì—¬ì„± 30ëŒ€', 'ì—¬ì„± 40ëŒ€', 'ì—¬ì„± 50ëŒ€', 'ì—¬ì„± 60ëŒ€ì´ìƒ',
    'ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘', 'ì‹ ê·œ ê³ ê° ë¹„ì¤‘', 'ê±°ì£¼ ì¸êµ¬ ê³ ê° ë¹„ìœ¨', 'ì§ì¥ ì´ìš© ê³ ê° ë¹„ìœ¨', 'ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨',
    'ë™ì¼ ìƒê¶Œ ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘'
    # í•„ìš”í•œ ë³€ìˆ˜ëª… ì¶”ê°€
]

for col in cols_to_replace:
    final_df[col] = final_df[col].replace([-999999.9, -999999, -99999.9, -99999], np.nan)

# ë³€í™˜ ê²°ê³¼ í™•ì¸
print(final_df[cols_to_replace].isnull().sum())

# ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ ì¶œë ¥
print(final_df.isnull().sum())

drop_cols = ['ë¸Œëœë“œêµ¬ë¶„ì½”ë“œ', 'ìƒê¶Œ', 'íì—…ì¼', 'ë°°ë‹¬ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨', 'ë™ì¼ ìƒê¶Œ ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘']
final_df = final_df.drop(columns=drop_cols)

final_df = final_df.dropna()

final_df.shape

final_df = final_df.reset_index(drop=True)

print(final_df.isnull().sum().sum())

"""# ğŸ“Œ íšŒë³µíƒ„ë ¥ì„± ì§€ìˆ˜ ë°˜ì˜"""

final_df.info()

grouped = final_df.groupby('ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸')

"""## 1) ì¬ë¬´ì  íƒ„ë ¥ì„±"""

def calc_financial_resilience(group):
    sales = group['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„']
    std_sales = np.std(sales.pct_change().dropna())  # ì›”ë³„ ë³€ë™ë¥  í‘œì¤€í¸ì°¨
    loyal_base = group['ê±°ì£¼ ì¸êµ¬ ê³ ê° ë¹„ìœ¨'].mean()
    repeat_ratio = group['ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘'].mean()
    # ì ìˆ˜í™”: (ì˜ˆì‹œ ê¸°ì¤€, ì‹¤ì œ í”„ë¡œì íŠ¸ ê¸°ì¤€ì— ë§ê²Œ ì¡°ì •)
    # ë§¤ì¶œ ë³€ë™ ì•ˆì •ì„± (ì‘ì„ìˆ˜ë¡ ë†’ìŒ)
    sales_score = np.select(
        [std_sales < 0.1, std_sales < 0.3],
        [5, 3], default=1
    )
    # ê±°ì£¼ ê³ ê° ë¹„ìœ¨
    loyal_score = np.select(
        [loyal_base >= 0.2, loyal_base >= 0.1],
        [5, 3], default=1
    )
    # ì¬ë°©ë¬¸ë¥ 
    repeat_score = np.select(
        [repeat_ratio >= 0.05, repeat_ratio >= 0.02],
        [5, 3], default=1
    )
    return pd.Series({
        'financial_score': (sales_score + loyal_score + repeat_score) / 3
    })

"""## 2) ìš´ì˜ì  íƒ„ë ¥ì„±"""

def calc_operational_resilience(group):
    potential_expansion = group['ìœ ë™ì¸êµ¬ ì´ìš© ê³ ê° ë¹„ìœ¨'].mean()
    sales_growth = (group['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„'].values[-1] - group['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„'].values[0]) / max(1, group['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„'].values[0])
    transaction_std = np.std(group['ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„'])
    potential_score = np.select(
        [potential_expansion > 0.15, potential_expansion > 0.05],
        [5, 3], default=1
    )
    profit_score = np.select(
        [sales_growth > 0.0, sales_growth > -0.1],
        [5, 3], default=1
    )
    transaction_score = np.select(
        [transaction_std <= 0.1 * group['ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„'].mean(), transaction_std <= 0.3 * group['ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„'].mean()],
        [5, 3], default=1
    )
    return pd.Series({
        'operational_score': (potential_score + profit_score + transaction_score) / 3
    })

"""## 3) ê´€ê³„ì  íƒ„ë ¥ì„±"""

def calc_relational_resilience(group):
    retention = group['ì¬ë°©ë¬¸ ê³ ê° ë¹„ì¤‘'].mean()
    # ì´ ì˜ˆì‹œì—ì„œëŠ” ìƒê¶Œê´€ê³„ì„±, ìœ„ê¸°ì ì‘ì„± ê´€ë ¨ ë³€ìˆ˜ê°€ ì—†ìœ¼ë¯€ë¡œ retention ì ìˆ˜ë§Œ ì ìš©
    retention_score = np.select(
        [retention >= 0.05, retention >= 0.02],
        [5, 3], default=1
    )
    # ì¶”ê°€ì •ë³´ ìˆë‹¤ë©´ 2-3ê°œ ë³€ìˆ˜ í‰ê· 
    return pd.Series({
        'relational_score': retention_score  # ë‹¨ì¼ ë³€ìˆ˜ ì˜ˆì‹œ
    })

"""## 4) ìµœì¢… íšŒë³µíƒ„ë ¥ì„± ì§€ìˆ˜ ê³„ì‚°"""

financial = grouped.apply(calc_financial_resilience)
operational = grouped.apply(calc_operational_resilience)
relational = grouped.apply(calc_relational_resilience)

result = pd.concat([financial, operational, relational], axis=1)

# ìµœì¢… íšŒë³µíƒ„ë ¥ì„± ì§€ìˆ˜
result['íšŒë³µíƒ„ë ¥ì„±ì§€ìˆ˜'] = (
    result['financial_score'] * 0.4 +
    result['operational_score'] * 0.35 +
    result['relational_score'] * 0.25
)

# ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— merge
final_df = final_df.merge(
    result[['financial_score', 'operational_score', 'relational_score', 'íšŒë³µíƒ„ë ¥ì„±ì§€ìˆ˜']],
    left_on='ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', right_index=True, how='left'
)

final_df.head()

df_selected = final_df[['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'financial_score', 'operational_score', 'relational_score', 'íšŒë³µíƒ„ë ¥ì„±ì§€ìˆ˜']]

df_selected.head()

print(df_selected.dtypes)

for col in df_selected.columns:
    print(
        col,
        df_selected[col].apply(lambda x: isinstance(x, (np.ndarray, list))).sum(),
        "ê±´ì´ ë°°ì—´ì´ë‚˜ ë¦¬ìŠ¤íŠ¸"
    )

def safe_float(x):
    if isinstance(x, (np.ndarray, list)):
        # ndarrayì¼ ê²½ìš°, 0ì°¨ì›ì´ë©´ item() ì‚¬ìš©, 1ì°¨ì›ì´ë©´ [0] ì‚¬ìš©
        if isinstance(x, np.ndarray):
            if x.ndim == 0:
                return float(x.item())
            else:
                return float(x[0])
        else: # listë¼ë©´
            return float(x[0])
    else:
        return float(x)

df_selected['relational_score'] = df_selected['relational_score'].apply(safe_float)

df_unique = df_selected.drop_duplicates()

df_unique.head()

df_unique.info()

df_unique.describe()

df_unique.to_csv('/content/sample_data/resilience_all.csv', index=False, encoding='utf-8-sig')

# ê¸°ì¤€ë…„ì›” ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ (202301~202412 ì‚¬ì´ ë°ì´í„°ë§Œ)
# start_ym = 202301
# end_ym = 202412

# cond = (final_df['ê¸°ì¤€ë…„ì›”'] >= start_ym) & (final_df['ê¸°ì¤€ë…„ì›”'] <= end_ym)
# filtered = final_df[cond].copy()

# ê°€ë§¹ì ë³„ë¡œ ê¸°ì¤€ë…„ì›” ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
# final_df = filtered.sort_values(['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'ê¸°ì¤€ë…„ì›”']).reset_index(drop=True)

# final_df.head()

# cols = ['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸', 'financial_score', 'operational_score', 'relational_score', 'íšŒë³µíƒ„ë ¥ì„±ì§€ìˆ˜']
# resilience_all = final_df[cols]
# resilience_all.to_csv('resilience_all.csv', index=False)

print"(

"""# ğŸ“Œ 'ê°€ë§¹ì ëª…'ì„ ì„ë² ë”©í•˜ì—¬ ë²¡í„°ë””ë¹„ ë§Œë“¤ê¸°"""

!pip install langchain langchain-community langchain-openai chromadb tiktoken -q

from langchain.document_loaders import PyMuPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain.embeddings import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

sample_df = final_df.iloc[::24].copy()

text_lst = []
store_nm= []
for idx, row in sample_df.iterrows():
    store_nm.append(row['ê°€ë§¹ì ëª…'])
    text = (
        f"êµ¬ë¶„ë²ˆí˜¸ {row['ê°€ë§¹ì êµ¬ë¶„ë²ˆí˜¸']}, "
        f"ì£¼ì†Œ {row['ê°€ë§¹ì ì£¼ì†Œ']}, "
        f"ê°€ë§¹ì ëª… {row['ê°€ë§¹ì ëª…']}, "
        f"ì§€ì—­ {row['ê°€ë§¹ì ì§€ì—­']}, "
        f"ì—…ì¢… {row['ì—…ì¢…']}, "
        f"ê°œì„¤ì¼ {row['ê°œì„¤ì¼']}, "
        f"ê¸°ì¤€ë…„ì›” {row['ê¸°ì¤€ë…„ì›”']}, "
        f"ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„ {row['ê°€ë§¹ì  ìš´ì˜ê°œì›”ìˆ˜ êµ¬ê°„']}ê°œì›”, "
        f"ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„ {row['ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„']}, "
        f"ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„ {row['ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„']}, "
        f"ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„ {row['ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„']}, "
        f"ê°ë‹¨ê°€ êµ¬ê°„ {row['ê°ë‹¨ê°€ êµ¬ê°„']}, "
        f"ì·¨ì†Œìœ¨ êµ¬ê°„ {row['ì·¨ì†Œìœ¨ êµ¬ê°„']:.1f}. "
        f"ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨ {row['ë™ì¼ ì—…ì¢… ë§¤ì¶œê¸ˆì•¡ ë¹„ìœ¨']:.1f}, "
        f"ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨ {row['ë™ì¼ ì—…ì¢… ë§¤ì¶œê±´ìˆ˜ ë¹„ìœ¨']:.1f}, "
        f"ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨ {row['ë™ì¼ ì—…ì¢… ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨']:.1f}, "
        f"ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨ {row['ë™ì¼ ìƒê¶Œ ë‚´ ë§¤ì¶œìˆ˜ìœ„ë¹„ìœ¨']:.1f}, "
        f"ë™ì¼ ì—…ì¢… ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘ {row['ë™ì¼ ì—…ì¢… ë‚´ í•´ì§€ ê°€ë§¹ì  ë¹„ì¤‘']:.1f}"
    )
    text_lst.append(text)

vectorstore = Chroma(collection_name="store_collection2", embedding_function=embedding_model)
vectorstore.add_texts(texts=store_nm, metadatas=[{"description": d} for d in text_lst])

import os
def load_api_keys(filepath="api_key.txt"):
    with open(filepath, "r") as f:
        for line in f:
            line = line.strip()
            if line and "=" in line:
                key, value = line.split("=", 1)
                os.environ[key.strip()] = value.strip()



# API í‚¤ ë¡œë“œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
load_api_keys('sample_data/api_key.txt')

"""### ì¿¼ë¦¬ ê²€ìƒ‰"""

query_name = "ëª©í¬**"
top_k = 3
results = vectorstore.similarity_search(query_name, k=top_k)

for i in range(top_k):
  print(results[i].metadata['description'])

store_nm