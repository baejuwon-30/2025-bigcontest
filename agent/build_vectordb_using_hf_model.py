# -*- coding: utf-8 -*-
"""Build_VectorDB_using_HF_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EyUw1tyw72dvKrHLrUDvTyws46Whrz30
"""

from google.colab import drive
import os
import pandas as pd
import json

import numpy as np

df1_store = pd.read_csv('sample_data/big_data_set1_f.csv', encoding='cp949')
df2 = pd.read_csv('sample_data/big_data_set2_f.csv', encoding='cp949')
df3 = pd.read_csv('sample_data/big_data_set3_f.csv', encoding='cp949')

column_map_df1 = {
    'ENCODED_MCT': '가맹점구분번호',
    'MCT_BSE_AR': '가맹점주소',
    'MCT_NM': '가맹점명',
    'MCT_BRD_NUM': '브랜드구분코드',
    'MCT_SIGUNGU_NM': '가맹점지역',
    'HPSN_MCT_ZCD_NM': '업종',
    'HPSN_MCT_BZN_CD_NM': '상권',
    'ARE_D': '개설일',
    'MCT_ME_D': '폐업일'}
df1_store = df1_store.rename(columns=column_map_df1)

df1_store.head()

"""회복탄력성 데이터 불러오기"""

df_res = pd.read_csv('sample_data/resilience_all.csv')

df_res.head()

"""회복탄력성 데이터 Join 하기"""

df1 = pd.merge(df1_store, df_res, how='left', on='가맹점구분번호')

"""Join 후 NaN 은 -999999.99 로 채우기"""

df1['financial_score'] = df1['financial_score'].fillna(-999999.99)
df1['operational_score'] = df1['operational_score'].fillna(-999999.99)
df1['relational_score'] = df1['relational_score'].fillna(-999999.99)
df1['회복탄력성지수'] = df1['회복탄력성지수'].fillna(-999999.99)

df1.head()

df_joined = pd.merge(df2, df3, on=['ENCODED_MCT', 'TA_YM'], how='inner')
df_joined['MCT_OPE_MS_CN'] = df_joined['MCT_OPE_MS_CN'].str[2:]
df_joined['RC_M1_SAA'] = df_joined['RC_M1_SAA'].str[2:]
df_joined['RC_M1_TO_UE_CT'] = df_joined['RC_M1_TO_UE_CT'].str[2:]
df_joined['RC_M1_UE_CUS_CN'] = df_joined['RC_M1_UE_CUS_CN'].str[2:]
df_joined['RC_M1_AV_NP_AT'] = df_joined['RC_M1_AV_NP_AT'].str[2:]
df_joined['APV_CE_RAT'] = df_joined['APV_CE_RAT'].str[2:]

column_map_join = {
    'ENCODED_MCT': '가맹점구분번호',
    'TA_YM': '기준년월',
    'MCT_OPE_MS_CN': '가맹점 운영개월수 구간',
    'RC_M1_SAA': '매출금액 구간',
    'RC_M1_TO_UE_CT': '매출건수 구간',
    'RC_M1_UE_CUS_CN': '유니크 고객 수 구간',
    'RC_M1_AV_NP_AT': '객단가 구간',
    'APV_CE_RAT': '취소율 구간',
    'DLV_SAA_RAT': '배달매출금액 비율',
    'M1_SME_RY_SAA_RAT': '동일 업종 매출금액 비율',
    'M1_SME_RY_CNT_RAT': '동일 업종 매출건수 비율',
    'M12_SME_RY_SAA_PCE_RT': '동일 업종 내 매출수위비율',
    'M12_SME_BZN_SAA_PCE_RT': '동일 상권 내 매출수위비율',
    'M12_SME_RY_ME_MCT_RAT': '동일 업종 내 해지 가맹점 비중',
    'M12_SME_BZN_ME_MCT_RAT': '동일 상권 내 해지 가맹점 비중',
    'M12_MAL_1020_RAT': '남성 20대이하',
    'M12_MAL_30_RAT': '남성 30대',
    'M12_MAL_40_RAT': '남성 40대',
    'M12_MAL_50_RAT': '남성 50대',
    'M12_MAL_60_RAT': '남성 60대이상',
    'M12_FME_1020_RAT': '여성 20대이하',
    'M12_FME_30_RAT': '여성 30대',
    'M12_FME_40_RAT': '여성 40대',
    'M12_FME_50_RAT': '여성 50대',
    'M12_FME_60_RAT': '여성 60대이상',
    'MCT_UE_CLN_REU_RAT': '재방문 고객 비중',
    'MCT_UE_CLN_NEW_RAT': '신규 고객 비중',
    'RC_M1_SHC_RSD_UE_CLN_RAT': '거주 인구 고객 비율',
    'RC_M1_SHC_WP_UE_CLN_RAT': '직장 이용 고객 비율',
    'RC_M1_SHC_FLP_UE_CLN_RAT': '유동인구 이용 고객 비율'
}


df_joined = df_joined.rename(columns=column_map_join)

store_cd = df1['가맹점구분번호']
store_cd_list_total = store_cd.tolist()
print(len(store_cd_list_total))

k = 2000
store_cd_list1 = store_cd_list_total[:k]
store_cd_list2 = store_cd_list_total[k:]

st_final_lst1=[]
st_final_lst2=[]

for cd in store_cd_list1:
  df_info = df1[df1['가맹점구분번호']==cd]
  st_info_dict_list = df_info.to_dict(orient='list')
  st_info_dict = {
    key: value[0]
    for key, value in st_info_dict_list.items()
  }


  df_usage = df_joined[df_joined['가맹점구분번호']==cd].sort_values(by='기준년월')
  df_usage_filtered = df_usage[['가맹점 운영개월수 구간','매출금액 구간','매출건수 구간','유니크 고객 수 구간','객단가 구간','취소율 구간','배달매출금액 비율','동일 업종 매출금액 비율', \
                          '동일 업종 매출건수 비율','동일 업종 내 매출수위비율','동일 상권 내 매출수위비율','동일 업종 내 해지 가맹점 비중','동일 상권 내 해지 가맹점 비중',\
                          '남성 20대이하','남성 30대','남성 40대','남성 50대','남성 60대이상','여성 20대이하','여성 30대','여성 40대','여성 50대','여성 60대이상','재방문 고객 비중',\
                          '신규 고객 비중','거주 인구 고객 비율','직장 이용 고객 비율','유동인구 이용 고객 비율']]
  st_usage_dict = df_usage_filtered.to_dict(orient='list')

  json_final_dict = st_info_dict | st_usage_dict
  json_final = json.dumps(json_final_dict, ensure_ascii=False, indent=4)
  st_final_lst1.append(json_final)



for cd in store_cd_list2:
  df_info = df1[df1['가맹점구분번호']==cd]
  st_info_dict_list = df_info.to_dict(orient='list')
  st_info_dict = {
    key: value[0]
    for key, value in st_info_dict_list.items()
  }


  df_usage = df_joined[df_joined['가맹점구분번호']==cd].sort_values(by='기준년월')
  df_usage_filtered = df_usage[['가맹점 운영개월수 구간','매출금액 구간','매출건수 구간','유니크 고객 수 구간','객단가 구간','취소율 구간','배달매출금액 비율','동일 업종 매출금액 비율', \
                          '동일 업종 매출건수 비율','동일 업종 내 매출수위비율','동일 상권 내 매출수위비율','동일 업종 내 해지 가맹점 비중','동일 상권 내 해지 가맹점 비중',\
                          '남성 20대이하','남성 30대','남성 40대','남성 50대','남성 60대이상','여성 20대이하','여성 30대','여성 40대','여성 50대','여성 60대이상','재방문 고객 비중',\
                          '신규 고객 비중','거주 인구 고객 비율','직장 이용 고객 비율','유동인구 이용 고객 비율']]
  st_usage_dict = df_usage_filtered.to_dict(orient='list')

  json_final_dict = st_info_dict | st_usage_dict
  json_final = json.dumps(json_final_dict, ensure_ascii=False, indent=4)
  st_final_lst2.append(json_final)

print(len(st_final_lst1))
print(len(st_final_lst2))

store_nm = df1['가맹점명']

store_nm_lst_tmp1 = store_nm[:k].tolist()
store_nm_lst1 = [s[:4] if len(s) > 4 else s for s in store_nm_lst_tmp1]
print(store_nm_lst1)
print(len(store_nm_lst1))

store_nm_lst_tmp2 = store_nm[k:].tolist()
store_nm_lst2 = [s[:4] if len(s) > 4 else s for s in store_nm_lst_tmp2]
print(store_nm_lst2)
print(len(store_nm_lst2))

!pip install langchain langchain-community pymilvus langchain_huggingface

from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from pymilvus import MilvusClient, CollectionSchema, FieldSchema, DataType
from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
import time
from dotenv import load_dotenv

# .env 파일에서 환경 변수(api key, URL 등) 로드
load_dotenv(dotenv_path='/content/sample_data/.env')

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
ZILLIZ_URI = os.getenv("ZILLIZ_URI")
ZILLIZ_TOKEN = os.getenv("ZILLIZ_TOKEN")
HF_TOKEN = os.environ.get("HF_TOKEN")

EMBEDDING_MODEL = "jhgan/ko-sroberta-nli"
EMBEDDING_DIMENSION =768
COLLECTION_NAME='shinahn_collection_hf'

"""임베딩 모델 테스트"""

# LangChain용 Hugging Face Endpoint Embedding 생성
embedding_model = HuggingFaceEndpointEmbeddings(
    model=EMBEDDING_MODEL,
    # task="feature-extraction",
    huggingfacehub_api_token=HF_TOKEN,
)

# 텍스트 리스트
texts = [
    "LangChain seamlessly integrates with Hugging Face API.",
    "Embeddings help capture semantic meaning of text."
]

# 임베딩 생성
vectors = embedding_model.embed_documents(texts)

"""벡터디비 생성 준비"""

try:
    # Zilliz Cloud (MilvusClient) 연결
    milvus_client = MilvusClient(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)
    print("✅ Zilliz Cloud에 연결되었습니다.")

    # LangChain OpenAIEmbeddings 초기화 (🚨 변경된 부분)
    # LangChain은 자동으로 OPENAI_API_KEY 환경 변수를 확인합니다.
    # 여기서는 명시적으로 key와 model을 전달합니다.
    # embedding_model = OpenAIEmbeddings(
    #     api_key=OPENAI_API_KEY,
    #     model=EMBEDDING_MODEL,
    #     # 'dimensions' 파라미터를 사용하여 벡터 길이를 1536으로 고정합니다.
    #     dimensions=EMBEDDING_DIMENSION
    # )
    embedding_model = HuggingFaceEndpointEmbeddings(
        model=EMBEDDING_MODEL,
        # task="feature-extraction",
        huggingfacehub_api_token=HF_TOKEN,
    )

    print(f"✅ LangChain OpenAIEmbeddings ({EMBEDDING_MODEL}) 클라이언트가 초기화되었습니다.")

except Exception as e:
    print(f"❌ 클라이언트 초기화 실패: {e}")
    exit()

# 필드 정의
pk_field = FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True)
text_field = FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=512)
# dim=1536은 text-embedding-3-small에 맞게 고정
vector_field = FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIMENSION)
description_field = FieldSchema(name="description", dtype=DataType.VARCHAR, max_length=65535)

# 컬렉션 스키마
schema = CollectionSchema(
    fields=[pk_field, text_field, vector_field, description_field],
    description="신한 관련 정보를 위한 text-embedding-3-small 컬렉션"
)

if milvus_client.has_collection(collection_name=COLLECTION_NAME):
    milvus_client.drop_collection(collection_name=COLLECTION_NAME)
    print(f"🔄 기존 컬렉션 '{COLLECTION_NAME}'을(를) 삭제했습니다.")



milvus_client.create_collection(
    collection_name=COLLECTION_NAME,
    schema=schema,
    metric_type="COSINE"
)


index_params = milvus_client.prepare_index_params()
index_params.add_index(
    field_name="vector",
    index_type="HNSW",  # 인덱스 유형: HNSW, IVFFLAT 등
    metric_type="COSINE" # 컬렉션 생성 시 지정한 메트릭과 일치해야 함
)
milvus_client.create_index(
    collection_name=COLLECTION_NAME,
    index_params=index_params,
    timeout=300 # 인덱스 생성에 시간이 걸릴 수 있으므로 타임아웃을 늘릴 수 있습니다.
)

embeddings1 = embedding_model.embed_documents(store_nm_lst1)
embeddings2 = embedding_model.embed_documents(store_nm_lst2)

data_to_insert1 = []
for text_a, vector, text_b in zip(store_nm_lst1, embeddings1, st_final_lst1):
    data_to_insert1.append({
        "text": text_a,
        "vector": vector,
        "description": text_b
    })

data_to_insert2 = []
for text_a, vector, text_b in zip(store_nm_lst2, embeddings2, st_final_lst2):
    data_to_insert2.append({
        "text": text_a,
        "vector": vector,
        "description": text_b
    })

print(len(data_to_insert1))
print(len(data_to_insert2))

try:
    insert_result1 = milvus_client.insert(
        collection_name=COLLECTION_NAME,
        data=data_to_insert1
    )

    insert_result2 = milvus_client.insert(
        collection_name=COLLECTION_NAME,
        data=data_to_insert2
    )


except Exception as e:
    print(f"Error inserting data: {e}")

milvus_client.load_collection(collection_name=COLLECTION_NAME)
print(f"🔄 컬렉션 '{COLLECTION_NAME}' 로드 완료.")

"""여기서부터는 테스트를 위한 코드임"""

search_query = "성우**"
query_vector = embedding_model.embed_query(search_query)
search_vectors = [query_vector]
milvus_client = MilvusClient(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)
output_fields = ["text", "description"]
search_result = milvus_client.search(
    collection_name=COLLECTION_NAME,
    data=search_vectors,      # 검색할 쿼리 벡터
    limit=1,                  # 상위 2개의 결과만 가져옴
    output_fields=output_fields # ⬅️ 검색 결과에 포함할 필드 지정
)

search_result[0][0]['entity']['description']

"""Retrieve 한 값을 google LLM 모델에 전송하여 결과 확인하기"""

from google import genai
from google.genai.errors import APIError

gemini_client  = genai.Client()
MODEL_NAME = 'gemini-2.5-flash'

def retrieve_and_format_rag_input_with_description(user_query: str) -> str:

  query_vector = embedding_model.embed_query(user_query)
  search_vectors = [query_vector]

  output_fields = ["text", "description"]
  search_result = milvus_client.search(
      collection_name=COLLECTION_NAME,
      data=search_vectors,      # 검색할 쿼리 벡터
      limit=1,                  # 상위 2개의 결과만 가져옴
      output_fields=output_fields # ⬅️ 검색 결과에 포함할 필드 지정
  )

  result = search_result[0][0]['entity']['description']
  return result
    # 1. 유사성 검색 수행
    # results = vectorstore.similarity_search(query=user_query, k=1)

    # return results[0].metadata['description']

def generate_answer_with_description_rag(gemini_client, user_query: str):
    """
    검색된 컨텍스트(문서 내용 + description 메타데이터)를 기반으로 Gemini에게 답변을 요청합니다.
    """

    full_context = retrieve_and_format_rag_input_with_description(user_query )


    # 프롬프트 구성
    prompt = f"""
      아래의 데이터는 JSON 형식의 데이터로 특정 가맹점의 정보와 그 가맹점의 최근 24 개월간의 월별 이용 정보와 월별 이용 고객 정보 데이터이며 시간순으로 정렬이 되어 있어.
      - 값이 -999999.99 인 경우 정보가 존재하지 않는 경우임
      - '가맹점 운영 개월수 구간': 0% 에 가까울 수록 상위임(운영개월 수가 상위 임)
      - '매출금액 구간': 0% 에 가까울 수록 상위임(매출 금액이 상위 임)
      - '매출건수 구간': 0% 에 가까울 수록 상위임(매출 건수가 상위 임)
      - '유니크 고객 수 구간': 0% 에 가까울 수록 상위임(unique 고객 수가 상위임)
      - '객단가 구간': 0% 에 가까울 수록 상위임(객단가가 상위 임)
      - '취소율 구간': 1 구간에 가까울 수록 취소율이 낮음
      - '동일 업종 매출금액 비율': 동일 업종 매출 금액 평균 대비 해당 가맹점의 매출 금액 비율이며 평균과 동일할 경우 100 이야.
      - '동일 업종 매출건수 비율': 동일 업종 매출 건수 평균 대비 해당 가맹점의 매출 건수 비율이며 평균과 동일할 경우 100 이야.
      - '동일 업종 내 매출 순위 비율': ('업종 내 순위'/'업종 내 전체 가맹점'* 100) 을 계산한 값으로 0 에 가까울 수록 상위에 랭킹되는거야.
      - '동일 상권 내 매출 순위 비율': ('상권 내 순위'/'상권 내 전체 가맹점'* 100) 을 계산한 값으로 0 에 가까울 수록 상위에 랭킹되는거야.

      아래의 data 를 분석해서 해당 가맹점의 매출 전략을 제안해줘.

    {full_context}

    ---

    QUERY:
    {user_query}
    """

    response = gemini_client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
        config={
            "temperature": 0.1 # 사실 기반 답변을 위해 낮은 온도를 사용
        }
    )

    return response.text

user_query = "이혜*******"
final_answer = generate_answer_with_description_rag(
    gemini_client=gemini_client,
    user_query=user_query
)
print(f"--- 💡 Gemini의 최종 답변 (메타데이터 활용 RAG) ---\n{final_answer}")

[{'동대**': 'ㅇㄴㄹㅇㄴㄹㅇㅁㄴㄹㅇㄴㅁ' }, {'게키**':}]