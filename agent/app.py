# app.py â€” Store Agents (3 buttons) + Universal(JSONâ†’Gemini) + VS Leader(Prebuilt JSON explain)
# - (1) ìš°ë¦¬ ê°€ê²Œ ë°ì´í„° ë¶„ì„: Milvus(HF ì„ë² ë”©) RAG â†’ Gemini ë³´ê³ ì„œ(ë§ˆí¬ë‹¤ìš´)
# - (2) ì˜¨ë¼ì¸ ëŒ€ì¤‘ì„± ìŠ¤ì½”ì–´ ë¶„ì„: data/{ë§¤ì¥ëª…}_persona_report.json â†’ Geminiê°€ ì§€í‘œ/ì •ë ¬/ë§¤í•‘
# - (3) 1ìœ„ ê°€ê²Œ vs ìš°ë¦¬ ê°€ê²Œ: ì‚¬ì „ ìƒì„± data/{ë§¤ì¥ëª…}_vs_leader.json ì„¤ëª…(LLM í˜¸ì¶œ ì—†ìŒ)

import json
import unicodedata
from pathlib import Path
import streamlit as st
from jinja2 import Template

# ================== Page ==================
st.set_page_config(page_title="ì„¸ ê°€ì§€ ì—ì´ì „íŠ¸ë¡œ ê°€ê²Œ ë¶„ì„í•˜ê¸°", page_icon="ğŸ§­", layout="centered")
st.title("ì„¸ ê°€ì§€ ì—ì´ì „íŠ¸ë¡œ ìš°ë¦¬ ê°€ê²Œ ë¶„ì„í•˜ê¸°")

# ================== Paths (agent/app.py ê¸°ì¤€ ê³ ì •) ==================
BASE_DIR = Path(__file__).parent          # .../agent
DATA_DIR = BASE_DIR / "data"              # .../agent/data

def nfc(s: str) -> str:
    """macOSâ†”Linux í•œê¸€ íŒŒì¼ëª… ì •ê·œí™” ì°¨ì´ ë°©ì§€"""
    return unicodedata.normalize("NFC", (s or "").strip())

def find_report_path(store_name: str) -> Path | None:
    """agent/dataì—ì„œ <ë§¤ì¥ëª…>_persona_report.json íƒìƒ‰"""
    target = nfc(store_name)
    exact = DATA_DIR / f"{target}_persona_report.json"
    if exact.exists():
        return exact
    if DATA_DIR.exists():
        for p in DATA_DIR.glob("*_persona_report.json"):
            stem = nfc(p.name.replace("_persona_report.json", ""))
            if stem.lower() == target.lower():
                return p
    return None

def find_vs_leader_path(store_name: str) -> Path | None:
    """agent/dataì—ì„œ <ë§¤ì¥ëª…>_vs_leader.json íƒìƒ‰"""
    target = nfc(store_name)
    exact = DATA_DIR / f"{target}_vs_leader.json"
    if exact.exists():
        return exact
    if DATA_DIR.exists():
        for p in DATA_DIR.glob("*_vs_leader.json"):
            stem = nfc(p.name.replace("_vs_leader.json", ""))
            if stem.lower() == target.lower():
                return p
    return None

# ================== Secrets ==================
GEMINI_API_KEY = (st.secrets.get("GEMINI_API_KEY") or "").strip()
ZILLIZ_URI     = (st.secrets.get("ZILLIZ_URI") or "").strip()
ZILLIZ_TOKEN   = (st.secrets.get("ZILLIZ_TOKEN") or "").strip()
HF_TOKEN       = (st.secrets.get("HF_TOKEN") or "").strip()

# ================== Cached clients ==================
@st.cache_resource(show_spinner=False)
def get_gemini_client():
    if not GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsì— ì¶”ê°€í•˜ì„¸ìš”.")
    try:
        import google.generativeai as genai  # ì§€ì—° import
    except ModuleNotFoundError as e:
        raise RuntimeError("google-generativeai íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. requirements.txtì— 'google-generativeai'ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.") from e
    genai.configure(api_key=GEMINI_API_KEY)
    return genai

@st.cache_resource(show_spinner=False)
def get_milvus():
    """Milvus í´ë¼ì´ì–¸íŠ¸ (ì—†ìœ¼ë©´ None ë°˜í™˜)"""
    if not (ZILLIZ_URI and ZILLIZ_TOKEN):
        return None
    try:
        from pymilvus import MilvusClient
    except ModuleNotFoundError as e:
        raise RuntimeError("pymilvus íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. requirements.txtì— 'pymilvus'ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.") from e
    return MilvusClient(uri=ZILLIZ_URI, token=ZILLIZ_TOKEN)

@st.cache_resource(show_spinner=False)
def get_embedding_model():
    """
    1) í—ˆë¸Œ(Endpoint) ì„ë² ë”© ì‹œë„: HF_TOKEN í•„ìš”
    2) ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ì„ë² ë”©ìœ¼ë¡œ í´ë°±(í† í° ë¶ˆí•„ìš”; transformers/torch í•„ìš”)
    """
    # 1) Hub Endpoint (í† í° í•„ìš”)
    if HF_TOKEN.startswith("hf_"):
        try:
            from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings
            return HuggingFaceEndpointEmbeddings(
                model="jhgan/ko-sroberta-nli",
                task="feature-extraction",
                huggingfacehub_api_token=HF_TOKEN,  # ì •í™•í•œ íŒŒë¼ë¯¸í„°ëª…
            )
        except Exception as e:
            st.warning(f"[ì„ë² ë”©] í—ˆë¸Œ API ì‹¤íŒ¨, ë¡œì»¬ ëª¨ë¸ë¡œ ì „í™˜í•©ë‹ˆë‹¤: {e}")

    # 2) Local (í† í° ë¶ˆí•„ìš”)
    try:
        from langchain_community.embeddings import HuggingFaceEmbeddings
        return HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-nli")
    except ModuleNotFoundError as e:
        raise RuntimeError(
            "ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨. í—ˆë¸Œ í† í°(HF_TOKEN) ë˜ëŠ” ë¡œì»¬ìš© íŒ¨í‚¤ì§€(transformers/sentence-transformers/torch)ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        ) from e

# ================== Constants (Milvus ìŠ¤í‚¤ë§ˆ ë§ì¶¤) ==================
STORE_ANALYSIS_COLLECTION = "shinahn_collection_hf"  # ë…¸íŠ¸ë¶/ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ
OUTPUT_FIELDS             = ["text", "description"]  # ìŠ¤í‚¤ë§ˆ í•„ë“œ

# ================== Utilities ==================
def embed_query(text: str):
    model = get_embedding_model()
    try:
        return model.embed_query(text)
    except Exception as e:
        raise RuntimeError("ì„ë² ë”© í˜¸ì¶œ ì‹¤íŒ¨. HF_TOKEN/ë„¤íŠ¸ì›Œí¬/íŒ¨í‚¤ì§€ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.") from e

def milvus_search_topk(query_vec, top_k=3, output_fields=None):
    milvus = get_milvus()
    if milvus is None:
        raise RuntimeError("Milvus ì—°ê²° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ZILLIZ_URI/ZILLIZ_TOKENì„ Secretsì— ì„¤ì •í•˜ì„¸ìš”.")
    output_fields = output_fields or OUTPUT_FIELDS
    res = milvus.search(
        collection_name=STORE_ANALYSIS_COLLECTION,
        data=[query_vec],
        limit=top_k,
        output_fields=output_fields,
        search_params={"metric_type": "COSINE"},
    )
    hits = []
    raw = res[0] if res else []
    for hit in raw:
        # MilvusClient search ê²°ê³¼ëŠ” dict í˜•íƒœ(entity í¬í•¨)ì¼ ìˆ˜ ìˆìŒ
        if isinstance(hit, dict):
            ent = hit.get("entity") or hit  # ì¼ë¶€ ë²„ì „ì€ ë°”ë¡œ í•„ë“œê°€ ìƒìœ„ì— ìˆìŒ
            row = {f: ent.get(f) for f in output_fields}
        else:
            row = {f: getattr(hit, f, None) for f in output_fields}
        hits.append(row)
    return hits

def build_context_text(hits, max_chars=4000):
    """text/descriptionì„ ë¬¶ì–´ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    pieces = []
    for i, h in enumerate(hits, 1):
        content = (h.get("text") or "").strip()
        desc    = (h.get("description") or "").strip()
        if not content and not desc:
            continue
        blob = f"[{i}] {content}"
        if desc:
            blob += f"\n(ì„¤ëª…: {desc})"
        pieces.append(blob)
    ctx = "\n\n".join(pieces)
    return ctx[:max_chars]

def load_prompt_template(name: str) -> Template:
    tmpl_path = BASE_DIR / "prompts" / f"{name}.jinja"
    text = tmpl_path.read_text(encoding="utf-8")
    return Template(text)

def call_gemini_text(prompt_text: str, model: str = "gemini-2.5-flash") -> str:
    client = get_gemini_client()
    model_instance = client.GenerativeModel(model)
    resp = model_instance.generate_content(prompt_text)
    return (resp.text or "").strip()

def call_gemini_json(prompt_text: str, model: str = "gemini-2.5-flash"):
    txt = call_gemini_text(prompt_text, model=model)
    s, e = txt.find("{"), txt.rfind("}")
    if s < 0 or e <= s:
        raise ValueError("Geminiê°€ JSONì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return json.loads(txt[s:e+1])

# ================== (1) Store Analysis: RAG í”„ë¡¬í”„íŠ¸ ==================
STORE_RAG_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì“°ëŠ” ë¦¬í…Œì¼ ë°ì´í„° ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” íŠ¹ì • ê°€ë§¹ì  ê´€ë ¨ RAG ì»¨í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤(ë¬¸ì„œ ë³¸ë¬¸ + ì„¤ëª… ë©”íƒ€). ì´ ì •ë³´ë¥¼ **ì‚¬ì‹¤ ìœ„ì£¼ë¡œ** ìš”ì•½í•˜ê³  ì‹¤í–‰ ì¡°ì–¸ì„ ì œì‹œí•˜ì„¸ìš”.

ë„ë©”ì¸ ê·œì¹™(ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ì§€í‘œ í•´ì„):
- ê°’ì´ -999999.99 ì¸ ê²½ìš°: **ê²°ì¸¡**(ì •ë³´ ì—†ìŒ)
- 'ê°€ë§¹ì  ìš´ì˜ ê°œì›”ìˆ˜ êµ¬ê°„': **0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„**
- 'ë§¤ì¶œê¸ˆì•¡ êµ¬ê°„': **0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„**
- 'ë§¤ì¶œê±´ìˆ˜ êµ¬ê°„': **0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„**
- 'ìœ ë‹ˆí¬ ê³ ê° ìˆ˜ êµ¬ê°„': **0%ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìƒìœ„**
- ë‚ ì§œ/ì›”ë³„ ì§€í‘œëŠ” **ìµœê·¼ 24ê°œì›”ì˜ ì¶”ì„¸**ê°€ ì¤‘ìš”. ì´ìƒì¹˜ëŠ” -999999.99ë¡œ í‘œê¸°ë  ìˆ˜ ìˆìŒ.

ì¶œë ¥ í˜•ì‹(ë§ˆí¬ë‹¤ìš´, í•œêµ­ì–´):
# ê°€ë§¹ì  ìŠ¤ëƒ…ìƒ·
- í•µì‹¬ ìš”ì•½ 3ì¤„(ë§¤ì¶œ/ë°©ë¬¸/ê³ ê°í’€ ê´€ì )
- ìµœê·¼ ì¶”ì„¸(ì¦ê°€/ì •ì²´/ê°ì†Œ) í•œ ì¤„

# ê°•ì ê³¼ ë¦¬ìŠ¤í¬
- **ê°•ì (3~5)**: ê·¼ê±°ë¥¼ ì§§ê²Œ
- **ë¦¬ìŠ¤í¬(3 ì´í•˜)**: ê·¼ê±°/ì˜í–¥ì„ ì§§ê²Œ

# ì‹¤í–‰ ì œì•ˆ(ìš°ì„ ìˆœìœ„ ìˆœ, 3~5ê°œ)
- {êµ¬ì²´ì  ì•¡ì…˜} â€” {ê¸°ëŒ€íš¨ê³¼/ì¸¡ì •ì§€í‘œ}

# ì°¸ê³  ì»¨í…ìŠ¤íŠ¸(ìš”ì•½)
- [1]~[3]ì—ì„œ ê·¼ê±°ê°€ ëœ ë¬¸êµ¬ë¥¼ **í•œ ì¤„ì”©** ìš”ì•½

ê¸ˆì§€:
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì‚¬ì‹¤/ìˆ˜ì¹˜ **ì¶”ê°€ë¡œ ë§Œë“¤ì§€ ë§ ê²ƒ**
- ì§€ë‚˜ì¹˜ê²Œ ì¼ë°˜ì ì¸ ì¡°ì–¸ ê¸ˆì§€(ì´ ê°€ë§¹ì ì˜ ë§¥ë½ ë°˜ì˜)
- ë‚´ë¶€ ì§€í‘œ ì •ì˜ë¥¼ ë°”ê¾¸ì§€ ë§ ê²ƒ

[ì»¨í…ìŠ¤íŠ¸ ì‹œì‘]
{context}
[ì»¨í…ìŠ¤íŠ¸ ë]

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"
""".strip()

def run_agent_store_analysis_report(user_query: str):
    """ì„ë² ë”©â†’Milvusê²€ìƒ‰â†’ì»¨í…ìŠ¤íŠ¸â†’Gemini ë³´ê³ ì„œ ìƒì„±"""
    vec  = embed_query(user_query)
    hits = milvus_search_topk(vec, top_k=3, output_fields=OUTPUT_FIELDS)
    ctx  = build_context_text(hits)
    prompt = STORE_RAG_PROMPT.format(context=ctx, question=user_query)
    report_md = call_gemini_text(prompt)
    return report_md, hits

# ================== (2) Universal: JSON â†’ Gemini ê³„ì‚°/ì •ë ¬/ë§¤í•‘ ==================
PERSONA_AGENT_PROMPT = """
You are a data analyst. You will receive ONE JSON report file that contains:
- store_name
- universal_appeal_score (precomputed)
- personas: { persona_id: {
    label, scores: {keyword_sum, ksf_sum, total_sum, reviews},
    pros[], cons[], suggestions[]
}}

Your tasks (DO THEM EXACTLY):
1) Read all personas. Compute:
   - appeal = average of all persona total_sum, rounded to 2 decimals
   - balance_cv = coefficient of variation of total_sum across personas (std/mean; population std), float
   - coverage = fraction of personas with total_sum > 0  (0~1 float)
2) Sort personas by total_sum DESC and return each with:
   id, label, total_sum, reviews, per_review = total_sum/reviews,
   pros[], cons[], suggestions[]
3) Build a 1:1 mapping between cons and suggestions (pair semantically; dedupe exact duplicates).
4) Return STRICT JSON ONLY:

{
  "metrics": {"appeal": 0.00, "balance_cv": 0.0, "coverage": 0.0},
  "personas_sorted": [
    {"id":"P40M","label":"40ëŒ€ ë‚¨ì„±","total_sum":0,"reviews":0,"per_review":0.0,
     "pros":["..."],"cons":["..."],"suggestions":["..."]}
  ],
  "action_map":[{"con":"...","suggestion":"..."}]
}

=== JSON REPORT START ===
"""

def run_agent_universal(store_name: str):
    json_path = find_report_path(store_name)
    if not json_path:
        existing = ", ".join(sorted([p.name for p in (DATA_DIR.glob("*_persona_report.json") if DATA_DIR.exists() else [])])[:50])
        raise FileNotFoundError(
            f"ë¦¬í¬íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: agent/data/{store_name}_persona_report.json\n"
            f"agent/data ë‚´ íŒŒì¼ë“¤: {existing or '(ì—†ìŒ)'}"
        )
    raw = json_path.read_text(encoding="utf-8")
    prompt = PERSONA_AGENT_PROMPT + raw + "\n=== JSON REPORT END ==="
    return call_gemini_json(prompt)

def render_persona_dashboard(result: dict):
    st.subheader("ê³ ê° ë§¤ë ¥ë„ ì¢…í•© ë¦¬í¬íŠ¸")
    m = result["metrics"]
    c1, c2, c3 = st.columns(3)
    c1.metric("ìš°ë¦¬ ê°€ê²Œê°€ ë°›ëŠ” ì¢…í•© í˜¸ê°ë„", f"{m['appeal']:.2f}")
    c2.metric("í˜ë¥´ì†Œë‚˜ë³„ í˜¸ê°ë„ ê· í˜•ì§€ìˆ˜", f"{m['balance_cv']:.4f}")
    c3.metric("í˜ë¥´ì†Œë‚˜ ê¸ì • ë°˜ì‘ë¥ ", f"{m['coverage']*100:.1f}%")

    st.subheader("í˜ë¥´ì†Œë‚˜ ìˆœìœ„ (total_sum ë‚´ë¦¼ì°¨ìˆœ)")
    for r in result["personas_sorted"]:
        header = f"{r['label']} â€” total {r['total_sum']}  (per review {r['per_review']:.3f}, reviews {r['reviews']})"
        with st.expander(header):
            st.write("**Pros**")
            for p in r.get("pros", []): st.write("- ", p)
            st.write("**Cons**")
            for c in r.get("cons", []): st.write("- ", c)
            st.write("**Suggestions**")
            for s in r.get("suggestions", []): st.write("- ", s)

    st.subheader("ë¬¸ì œ â†” í•´ê²° ì „ëµ 1:1 ë§¤í•‘")
    for pair in result.get("action_map", []):
        st.markdown(f"âš ï¸ **ë¬¸ì œ:** {pair['con']}  \nâ˜‘ï¸ **ì¡°ì¹˜:** {pair['suggestion']}")

# ================== VS Leader ë¹„êµ JSON ë Œë” (ì˜µì…˜ B: êµ¬ì¡°í™” ì¶œë ¥) ==================
def render_vs_leader_pack(pack: dict) -> None:
    """ì‚¬ì „ ìƒì„±ëœ ë¹„êµ JSON(dict) ì„¤ëª… â€” ë‚´ëŸ¬í‹°ë¸Œì—ì„œ [ê°œì„  ì œì•ˆ] ì œì™¸, actionsë§Œ 1íšŒ í‘œì‹œ"""
    store_our = pack.get("store_name_our", "ìš°ë¦¬ ê°€ê²Œ")
    store_leader = pack.get("store_name_leader", "1ìœ„ ê°€ê²Œ")
    st.subheader(f"ë¹„êµ ê²°ê³¼: {store_our} vs {store_leader}")

    st.subheader("í˜ë¥´ì†Œë‚˜ë³„ ê²©ì°¨ ë¶„ì„")
    personas = pack.get("personas", [])
    if not personas:
        st.info("í˜ë¥´ì†Œë‚˜ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for p in personas:
        label = p.get("label", p.get("persona_id", "ì•Œ ìˆ˜ ì—†ìŒ"))
        st.markdown(f"**{label}**")

        # ì ìˆ˜ ìš”ì•½
        s_our = p.get("score_ours")
        s_lead = p.get("score_leader")
        gap = p.get("gap")
        if s_our is not None and s_lead is not None and gap is not None:
            st.caption(f"ì ìˆ˜: ìš°ë¦¬ {s_our} / 1ìœ„ {s_lead} (gap {gap:+})")

        # ë‚´ëŸ¬í‹°ë¸Œ: [ê°œì„  ì œì•ˆ] ì´ì „ê¹Œì§€ë§Œ ì¶œë ¥
        narrative = (p.get("narrative") or "").strip()
        if narrative:
            text_no_actions = narrative.split("[ê°œì„  ì œì•ˆ]")[0].strip()
            if text_no_actions:
                st.write(text_no_actions)

        # ì•¡ì…˜: êµ¬ì¡°í™” ë°°ì—´ë¡œ 1íšŒ + ì¤‘ë³µ ì œê±°
        actions_in = p.get("actions") or []
        dedup, seen = [], set()
        for a in actions_in:
            s = (a or "").strip()
            if s and s not in seen:
                seen.add(s)
                dedup.append(s)
        if dedup:
            st.markdown("**[ê°œì„  ì œì•ˆ]**")
            for a in dedup:
                st.write("- ", a)

    roadmap = pack.get("roadmap") or []
    if roadmap:
        st.subheader("ìš°ì„  ì‹¤í–‰ ë¡œë“œë§µ")
        for line in roadmap:
            st.write(" ", line)

# ================== Agent selector (3 Buttons) ==================
if "agent" not in st.session_state:
    st.session_state.agent = None

AGENTS = {
    "store_analysis": {
        "name": "ğŸ” ìš°ë¦¬ ê°€ê²Œ ë°ì´í„° ë¶„ì„",
        "desc": "Milvus(HF ì„ë² ë”©) RAG ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ ë¬¸ë§¥ì„ ì°¾ì•„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
    },
    "universal": {
        "name": "ğŸŒ ì˜¨ë¼ì¸ ëŒ€ì¤‘ì„± ìŠ¤ì½”ì–´ ë¶„ì„",
        "desc": "Naver ì§€ë„ ë¦¬ë·° ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ê°€ê²Œì˜ 'ì˜¨ë¼ì¸ ëŒ€ì¤‘ì„±'ê³¼ 'ê³ ê°ì¸µë³„ ë§¤ë ¥ë„'ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤."
    },
    "vs_leader": {
        "name": "ğŸ† 1ìœ„ ê°€ê²Œ vs ìš°ë¦¬ ê°€ê²Œ ë¶„ì„",
        "desc": "JSONì„ ë¶ˆëŸ¬ì™€ ë¹„êµí•˜ì—¬ í˜ë¥´ì†Œë‚˜ë³„ ì ìˆ˜Â·ê°­Â·ê°œì„ ì•ˆì„ ì„¤ëª…í•©ë‹ˆë‹¤."
    },
}

st.markdown("#### ì—ì´ì „íŠ¸ ì„ íƒ")
c1, c2, c3 = st.columns(3)
with c1:
    if st.button(AGENTS["store_analysis"]["name"]):
        st.session_state.agent = "store_analysis"
with c2:
    if st.button(AGENTS["universal"]["name"]):
        st.session_state.agent = "universal"
with c3:
    if st.button(AGENTS["vs_leader"]["name"]):
        st.session_state.agent = "vs_leader"

agent = st.session_state.get("agent")
if not agent:
    st.info("ìƒë‹¨ ë²„íŠ¼ ì¤‘ í•˜ë‚˜ë¥¼ ëˆŒëŸ¬ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    st.stop()
else:
    st.success(AGENTS[agent].get("desc", AGENTS[agent]["name"]))

# ================== Branch: 2) Universal (JSON -> Gemini) ==================
if agent == "universal":
    store_in = st.chat_input("ê°€ê²Œëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: í¬ë ˆì†¡, ë™ì›ê°, ë°íŒì•¼ë¼í˜„, ëª…ê°€ë–¡ë³¶ì´ ë“±)")
    if store_in:
        with st.spinner("ìš°ë¦¬ ê°€ê²Œì˜ ì˜¨ë¼ì¸ ëŒ€ì¤‘ì„± ìŠ¤ì½”ì–´ ë¶„ì„ ì¤‘..."):
            try:
                result = run_agent_universal(store_in.strip())
                render_persona_dashboard(result)
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")

# ================== Branch: 3) VS Leader (ì‚¬ì „ ìƒì„±ëœ ë¹„êµ JSON ìë™ ë¡œë“œ) ==================
elif agent == "vs_leader":
    our_store = st.text_input("ìš°ë¦¬ ê°€ê²Œëª…", placeholder="ì˜ˆ: ë™ì›ê° / í¬ë ˆì†¡ / ë°íŒì•¼ë¼í˜„ / ëª…ê°€ë–¡ë³¶ì´")
    if st.button("ë¹„êµ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°", type="primary", use_container_width=True):
        try:
            if not our_store or not our_store.strip():
                raise ValueError("ìš°ë¦¬ ê°€ê²Œëª…ì„ ì…ë ¥í•˜ì„¸ìš”.")
            path = find_vs_leader_path(our_store.strip())
            if not path:
                existing = ", ".join(sorted([p.name for p in (DATA_DIR.glob("*_vs_leader.json") if DATA_DIR.exists() else [])])[:50])
                raise FileNotFoundError(
                    f"ë¹„êµ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: agent/data/{our_store}_vs_leader.json\n"
                    f"agent/data ë‚´ íŒŒì¼ë“¤: {existing or '(ì—†ìŒ)'}"
                )
            pack = json.loads(path.read_text(encoding="utf-8"))
            render_vs_leader_pack(pack)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")

# ================== Branch: 1) Store Analysis (Milvus RAG) ==================
elif agent == "store_analysis":
    q = st.chat_input("ê°€ë§¹ì  ì´ë¦„ ë˜ëŠ” ë¶„ì„ í¬ì»¤ìŠ¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë™ì›ê° ì ì˜ ì„±ì¥/ë¦¬ìŠ¤í¬ ìš”ì•½)")
    if q:
        with st.spinner("ìƒìœ„ ë¬¸ë§¥ ê²€ìƒ‰ ë° ë³´ê³ ì„œ ìƒì„± ì¤‘..."):
            try:
                report_md, hits = run_agent_store_analysis_report(q.strip())
                st.markdown(report_md)
                with st.expander("ì°¸ê³  ë¬¸ë§¥(ê²€ìƒ‰ ìƒìœ„ ê²°ê³¼)"):
                    for i, h in enumerate(hits, 1):
                        st.markdown(f"**[{i}]** {h.get('text','')}")
                        desc = h.get("description")
                        if desc:
                            st.caption(f"ì„¤ëª…: {desc}")
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {e}")
